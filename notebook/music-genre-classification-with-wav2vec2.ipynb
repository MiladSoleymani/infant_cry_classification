{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1032238,"sourceType":"datasetVersion","datasetId":568973}],"dockerImageVersionId":30163,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Wav2Vec 2.0** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by Alexei Baevski, Michael Auli, and Alex Conneau.  Soon after the superior performance of Wav2Vec2 was demonstrated on the English ASR dataset LibriSpeech, *Facebook AI* presented XLSR-Wav2Vec2 (click [here](https://arxiv.org/abs/2006.13979)). XLSR stands for *cross-lingual  speech representations* and refers to XLSR-Wav2Vec2`s ability to learn speech representations that are useful across multiple languages.\n\nSimilar to Wav2Vec2, XLSR-Wav2Vec2 learns powerful speech representations from hundreds of thousands of hours of speech in more than 50 languages of unlabeled speech. Similar, to [BERT's masked language modeling](http://jalammar.github.io/illustrated-bert/), the model learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network.\n\n![wav2vec2_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xlsr_wav2vec2.png)\n\nThe authors show for the first time that massively pretraining an ASR model on cross-lingual unlabeled speech data, followed by language-specific fine-tuning on very little labeled data achieves state-of-the-art results. See Table 1-5 of the official [paper](https://arxiv.org/pdf/2006.13979.pdf).","metadata":{}},{"cell_type":"code","source":"%%capture\n\n!pip install git+https://github.com/huggingface/datasets.git\n!pip install git+https://github.com/huggingface/transformers.git\n!pip install torchaudio\n!pip install librosa\n\n# Monitor the training process\n# !pip install wandb","metadata":{"execution":{"iopub.status.busy":"2022-02-21T18:29:18.744508Z","iopub.execute_input":"2022-02-21T18:29:18.74504Z","iopub.status.idle":"2022-02-21T18:30:40.477077Z","shell.execute_reply.started":"2022-02-21T18:29:18.744915Z","shell.execute_reply":"2022-02-21T18:30:40.475904Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%env LC_ALL=C.UTF-8\n%env LANG=C.UTF-8\n%env TRANSFORMERS_CACHE=/kaggle/working/cache\n%env HF_DATASETS_CACHE=/kaggle/working/cache\n%env CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2022-02-21T18:30:45.949075Z","iopub.execute_input":"2022-02-21T18:30:45.949702Z","iopub.status.idle":"2022-02-21T18:30:45.961752Z","shell.execute_reply.started":"2022-02-21T18:30:45.949653Z","shell.execute_reply":"2022-02-21T18:30:45.960807Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Set Variables","metadata":{}},{"cell_type":"code","source":"dataset_path = '../input/gtzan-dataset-music-genre-classification/Data'","metadata":{"execution":{"iopub.status.busy":"2022-02-21T18:30:50.070578Z","iopub.execute_input":"2022-02-21T18:30:50.071223Z","iopub.status.idle":"2022-02-21T18:30:50.075718Z","shell.execute_reply.started":"2022-02-21T18:30:50.071173Z","shell.execute_reply":"2022-02-21T18:30:50.074889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ignorefiles = ['jazz.00054.wav']","metadata":{"execution":{"iopub.status.busy":"2022-02-21T18:30:50.078132Z","iopub.execute_input":"2022-02-21T18:30:50.078601Z","iopub.status.idle":"2022-02-21T18:30:50.087099Z","shell.execute_reply.started":"2022-02-21T18:30:50.078439Z","shell.execute_reply":"2022-02-21T18:30:50.086558Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name_or_path = \"m3hrdadfi/wav2vec2-base-100k-gtzan-music-genres\"\n# model_name_or_path = \"models/wav2vec2-base-100k-gtzan-music-genres\"","metadata":{"execution":{"iopub.status.busy":"2022-02-21T18:30:50.417027Z","iopub.execute_input":"2022-02-21T18:30:50.417626Z","iopub.status.idle":"2022-02-21T18:30:50.422123Z","shell.execute_reply.started":"2022-02-21T18:30:50.417573Z","shell.execute_reply":"2022-02-21T18:30:50.421276Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\n\nimport transformers\nfrom transformers import (\n    HfArgumentParser,\n    TrainingArguments,\n    EvalPrediction,\n    AutoConfig,\n    Wav2Vec2Processor,\n    Wav2Vec2FeatureExtractor,\n    is_apex_available,\n    set_seed,\n)\nfrom transformers.trainer_utils import get_last_checkpoint, is_main_process\n\n\nimport os\nimport sys\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nimport torchaudio\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:33:31.777609Z","iopub.execute_input":"2022-02-21T04:33:31.777895Z","iopub.status.idle":"2022-02-21T04:33:31.784947Z","shell.execute_reply.started":"2022-02-21T04:33:31.777854Z","shell.execute_reply":"2022-02-21T04:33:31.78413Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## wav2vec2 model","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Optional, Tuple\nimport torch\nfrom transformers.file_utils import ModelOutput","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:33:46.488194Z","iopub.execute_input":"2022-02-21T04:33:46.488477Z","iopub.status.idle":"2022-02-21T04:33:46.49247Z","shell.execute_reply.started":"2022-02-21T04:33:46.488449Z","shell.execute_reply":"2022-02-21T04:33:46.491804Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclass\nclass SpeechClassifierOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:33:49.466764Z","iopub.execute_input":"2022-02-21T04:33:49.467194Z","iopub.status.idle":"2022-02-21T04:33:49.473853Z","shell.execute_reply.started":"2022-02-21T04:33:49.467159Z","shell.execute_reply":"2022-02-21T04:33:49.473089Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import (\n    Wav2Vec2PreTrainedModel,\n    Wav2Vec2Model\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:33:53.747164Z","iopub.execute_input":"2022-02-21T04:33:53.747849Z","iopub.status.idle":"2022-02-21T04:33:53.780615Z","shell.execute_reply.started":"2022-02-21T04:33:53.74781Z","shell.execute_reply":"2022-02-21T04:33:53.779915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Wav2Vec2ClassificationHead(nn.Module):\n    \"\"\"Head for wav2vec classification task.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.final_dropout)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        x = features\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n\n\nclass Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.pooling_mode = config.pooling_mode\n        self.config = config\n\n        self.wav2vec2 = Wav2Vec2Model(config)\n        self.classifier = Wav2Vec2ClassificationHead(config)\n\n        self.init_weights()\n\n    def freeze_feature_extractor(self):\n        self.wav2vec2.feature_extractor._freeze_parameters()\n\n    def merged_strategy(\n            self,\n            hidden_states,\n            mode=\"mean\"\n    ):\n        if mode == \"mean\":\n            outputs = torch.mean(hidden_states, dim=1)\n        elif mode == \"sum\":\n            outputs = torch.sum(hidden_states, dim=1)\n        elif mode == \"max\":\n            outputs = torch.max(hidden_states, dim=1)[0]\n        else:\n            raise Exception(\n                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n\n        return outputs\n\n    def forward(\n            self,\n            input_values,\n            attention_mask=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            labels=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.wav2vec2(\n            input_values,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0]\n        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n        logits = self.classifier(hidden_states)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SpeechClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:34:10.827274Z","iopub.execute_input":"2022-02-21T04:34:10.827532Z","iopub.status.idle":"2022-02-21T04:34:10.848112Z","shell.execute_reply.started":"2022-02-21T04:34:10.827503Z","shell.execute_reply":"2022-02-21T04:34:10.847175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare Dataset","metadata":{}},{"cell_type":"markdown","source":"### Read directory structure","metadata":{}},{"cell_type":"code","source":"data = []\n\nfor path in Path(f'{dataset_path}/genres_original').glob(\"**/*.wav\"):\n    if sys.platform == 'win32':\n        pathsep = '\\\\'\n    else:\n        pathsep = '/'\n        \n    label = str(path).split(pathsep)[-2]\n    name = str(path).split(pathsep)[-1]\n    \n    if name in ignorefiles:\n        continue\n\n    data.append({\n        # \"name\": name,\n        \"path\": path,\n        \"label\": label\n    })","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:34:20.190808Z","iopub.execute_input":"2022-02-21T04:34:20.191084Z","iopub.status.idle":"2022-02-21T04:34:20.535854Z","shell.execute_reply.started":"2022-02-21T04:34:20.191043Z","shell.execute_reply":"2022-02-21T04:34:20.535092Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(data)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:34:24.13709Z","iopub.execute_input":"2022-02-21T04:34:24.137876Z","iopub.status.idle":"2022-02-21T04:34:24.16075Z","shell.execute_reply.started":"2022-02-21T04:34:24.137832Z","shell.execute_reply":"2022-02-21T04:34:24.159944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.groupby(\"label\").count()[[\"path\"]]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:34:28.182224Z","iopub.execute_input":"2022-02-21T04:34:28.182506Z","iopub.status.idle":"2022-02-21T04:34:28.202748Z","shell.execute_reply.started":"2022-02-21T04:34:28.182476Z","shell.execute_reply":"2022-02-21T04:34:28.201647Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Split train and test set","metadata":{}},{"cell_type":"code","source":"save_path = \"/kaggle/working\"\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=101, stratify=df[\"label\"])\n\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\ntrain_df.to_csv(f\"{save_path}/train.csv\", encoding=\"utf-8\", index=False)\ntest_df.to_csv(f\"{save_path}/test.csv\", encoding=\"utf-8\", index=False)\n\n\nprint(train_df.shape)\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:34:31.386631Z","iopub.execute_input":"2022-02-21T04:34:31.386895Z","iopub.status.idle":"2022-02-21T04:34:31.409171Z","shell.execute_reply.started":"2022-02-21T04:34:31.386858Z","shell.execute_reply":"2022-02-21T04:34:31.408458Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, rest_df = train_test_split(df, test_size=0.2, random_state=101, stratify=df[\"label\"])\nvalid_df, test_df = train_test_split(rest_df, test_size=0.5, random_state=101, stratify=rest_df[\"label\"])\n\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\ntrain_df.to_csv(f\"{save_path}/train.csv\", encoding=\"utf-8\", index=False)\nvalid_df.to_csv(f\"{save_path}/valid.csv\", encoding=\"utf-8\", index=False)\ntest_df.to_csv(f\"{save_path}/test.csv\", encoding=\"utf-8\", index=False)\n\n\nprint(train_df.shape)\nprint(valid_df.shape)\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:34:33.924479Z","iopub.execute_input":"2022-02-21T04:34:33.925182Z","iopub.status.idle":"2022-02-21T04:34:33.948767Z","shell.execute_reply.started":"2022-02-21T04:34:33.925145Z","shell.execute_reply":"2022-02-21T04:34:33.948047Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare Data for Training","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, load_metric","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:34:36.906049Z","iopub.execute_input":"2022-02-21T04:34:36.906786Z","iopub.status.idle":"2022-02-21T04:34:37.152984Z","shell.execute_reply.started":"2022-02-21T04:34:36.906747Z","shell.execute_reply":"2022-02-21T04:34:37.152089Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_files = {\n    \"train\": f\"{save_path}/train.csv\", \n    \"validation\": f\"{save_path}/test.csv\",\n    # \"test\": f\"{dataset_path}/test.csv\",\n}\n\ndataset = load_dataset(\"csv\", data_files=data_files)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:34:39.82594Z","iopub.execute_input":"2022-02-21T04:34:39.826703Z","iopub.status.idle":"2022-02-21T04:34:40.336351Z","shell.execute_reply.started":"2022-02-21T04:34:39.826651Z","shell.execute_reply":"2022-02-21T04:34:40.33563Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = dataset[\"train\"]\neval_dataset = dataset[\"validation\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:34:42.946534Z","iopub.execute_input":"2022-02-21T04:34:42.947179Z","iopub.status.idle":"2022-02-21T04:34:42.951144Z","shell.execute_reply.started":"2022-02-21T04:34:42.947145Z","shell.execute_reply":"2022-02-21T04:34:42.950132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We need to specify the input and output column\ninput_column = \"path\"\noutput_column = \"label\"","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:36:02.028342Z","iopub.execute_input":"2022-02-21T04:36:02.02908Z","iopub.status.idle":"2022-02-21T04:36:02.032687Z","shell.execute_reply.started":"2022-02-21T04:36:02.02903Z","shell.execute_reply":"2022-02-21T04:36:02.031981Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# we need to distinguish the unique labels in our SER dataset\nlabel_list = train_dataset.unique(output_column)\nlabel_list.sort()  # Let's sort it for determinism\nnum_labels = len(label_list)\nprint(f\"A classification problem with {num_labels} classes:\\n {label_list}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:36:04.452188Z","iopub.execute_input":"2022-02-21T04:36:04.452728Z","iopub.status.idle":"2022-02-21T04:36:04.46036Z","shell.execute_reply.started":"2022-02-21T04:36:04.452689Z","shell.execute_reply":"2022-02-21T04:36:04.459434Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoConfig, Wav2Vec2Processor, Wav2Vec2FeatureExtractor","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:36:07.786474Z","iopub.execute_input":"2022-02-21T04:36:07.787183Z","iopub.status.idle":"2022-02-21T04:36:07.790972Z","shell.execute_reply.started":"2022-02-21T04:36:07.787146Z","shell.execute_reply":"2022-02-21T04:36:07.790263Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name_or_path = \"facebook/wav2vec2-base-100k-voxpopuli\"\npooling_mode = \"mean\"","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:36:12.406053Z","iopub.execute_input":"2022-02-21T04:36:12.406768Z","iopub.status.idle":"2022-02-21T04:36:12.412105Z","shell.execute_reply.started":"2022-02-21T04:36:12.40673Z","shell.execute_reply":"2022-02-21T04:36:12.411302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(\n    model_name_or_path,\n    num_labels=num_labels,\n    label2id={label: i for i, label in enumerate(label_list)},\n    id2label={i: label for i, label in enumerate(label_list)},\n    finetuning_task=\"wav2vec2_clf\",\n)\nsetattr(config, 'pooling_mode', pooling_mode)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:36:15.245693Z","iopub.execute_input":"2022-02-21T04:36:15.246425Z","iopub.status.idle":"2022-02-21T04:36:15.963116Z","shell.execute_reply.started":"2022-02-21T04:36:15.246388Z","shell.execute_reply":"2022-02-21T04:36:15.962396Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)\ntarget_sampling_rate = feature_extractor.sampling_rate\nprint(f\"The target sampling rate: {target_sampling_rate}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:36:26.08722Z","iopub.execute_input":"2022-02-21T04:36:26.088057Z","iopub.status.idle":"2022-02-21T04:36:26.80225Z","shell.execute_reply.started":"2022-02-21T04:36:26.088018Z","shell.execute_reply":"2022-02-21T04:36:26.801533Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocess Data","metadata":{}},{"cell_type":"code","source":"def speech_file_to_array_fn(path):\n    speech_array, sampling_rate = torchaudio.load(path)\n    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n    speech = resampler(speech_array).squeeze().numpy()\n    return speech\n\ndef label_to_id(label, label_list):\n\n    if len(label_list) > 0:\n        return label_list.index(label) if label in label_list else -1\n\n    return label\n\ndef preprocess_function(examples):\n    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n    target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n\n    result = feature_extractor(speech_list, sampling_rate=target_sampling_rate)\n    result[\"labels\"] = list(target_list)\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:36:30.052878Z","iopub.execute_input":"2022-02-21T04:36:30.05316Z","iopub.status.idle":"2022-02-21T04:36:30.062608Z","shell.execute_reply.started":"2022-02-21T04:36:30.053128Z","shell.execute_reply":"2022-02-21T04:36:30.061904Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = train_dataset.map(\n    preprocess_function,\n    batch_size=10,\n    batched=True,\n    # num_proc=4\n)\neval_dataset = eval_dataset.map(\n    preprocess_function,\n    batch_size=10,\n    batched=True,\n    # num_proc=4\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:36:32.298736Z","iopub.execute_input":"2022-02-21T04:36:32.299193Z","iopub.status.idle":"2022-02-21T04:37:43.7651Z","shell.execute_reply.started":"2022-02-21T04:36:32.299155Z","shell.execute_reply":"2022-02-21T04:37:43.764213Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Set Up Trainer","metadata":{"execution":{"iopub.status.busy":"2022-02-20T11:31:09.673208Z","iopub.execute_input":"2022-02-20T11:31:09.673675Z","iopub.status.idle":"2022-02-20T11:31:09.677805Z","shell.execute_reply.started":"2022-02-20T11:31:09.673638Z","shell.execute_reply":"2022-02-20T11:31:09.677068Z"}}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport torch\n\nimport transformers\nfrom transformers import Wav2Vec2Processor, Wav2Vec2FeatureExtractor","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:38:37.726383Z","iopub.execute_input":"2022-02-21T04:38:37.727142Z","iopub.status.idle":"2022-02-21T04:38:37.732259Z","shell.execute_reply.started":"2022-02-21T04:38:37.727094Z","shell.execute_reply":"2022-02-21T04:38:37.731365Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclass\nclass DataCollatorCTCWithPadding:\n    feature_extractor: Wav2Vec2FeatureExtractor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    max_length_labels: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    pad_to_multiple_of_labels: Optional[int] = None\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [feature[\"labels\"] for feature in features]\n\n        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n\n        batch = self.feature_extractor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n\n        return batch","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:38:39.367766Z","iopub.execute_input":"2022-02-21T04:38:39.368033Z","iopub.status.idle":"2022-02-21T04:38:39.377333Z","shell.execute_reply.started":"2022-02-21T04:38:39.368005Z","shell.execute_reply":"2022-02-21T04:38:39.376615Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_collator = DataCollatorCTCWithPadding(feature_extractor=feature_extractor, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:38:40.738195Z","iopub.execute_input":"2022-02-21T04:38:40.738686Z","iopub.status.idle":"2022-02-21T04:38:40.742436Z","shell.execute_reply.started":"2022-02-21T04:38:40.738647Z","shell.execute_reply":"2022-02-21T04:38:40.741582Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"is_regression = False","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:38:41.871753Z","iopub.execute_input":"2022-02-21T04:38:41.872162Z","iopub.status.idle":"2022-02-21T04:38:41.876102Z","shell.execute_reply.started":"2022-02-21T04:38:41.872128Z","shell.execute_reply":"2022-02-21T04:38:41.875125Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom transformers import EvalPrediction","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:38:42.827261Z","iopub.execute_input":"2022-02-21T04:38:42.827724Z","iopub.status.idle":"2022-02-21T04:38:42.831469Z","shell.execute_reply.started":"2022-02-21T04:38:42.827688Z","shell.execute_reply":"2022-02-21T04:38:42.830728Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n\n    if is_regression:\n        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n    else:\n        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:38:43.925801Z","iopub.execute_input":"2022-02-21T04:38:43.926533Z","iopub.status.idle":"2022-02-21T04:38:43.932338Z","shell.execute_reply.started":"2022-02-21T04:38:43.926494Z","shell.execute_reply":"2022-02-21T04:38:43.93151Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Wav2Vec2ForSpeechClassification.from_pretrained(\n    model_name_or_path,\n    config=config,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:38:45.365564Z","iopub.execute_input":"2022-02-21T04:38:45.365841Z","iopub.status.idle":"2022-02-21T04:38:56.939963Z","shell.execute_reply.started":"2022-02-21T04:38:45.365813Z","shell.execute_reply":"2022-02-21T04:38:56.938974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.freeze_feature_extractor()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:40:25.537904Z","iopub.execute_input":"2022-02-21T04:40:25.538512Z","iopub.status.idle":"2022-02-21T04:40:25.543126Z","shell.execute_reply.started":"2022-02-21T04:40:25.538474Z","shell.execute_reply":"2022-02-21T04:40:25.542371Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import Any, Dict, Union\n\nimport torch\nfrom packaging import version\nfrom torch import nn\n\nfrom transformers import (\n    Trainer,\n    is_apex_available,\n)\n\nif is_apex_available():\n    from apex import amp\n\nif version.parse(torch.__version__) >= version.parse(\"1.6\"):\n    _is_native_amp_available = True\n    from torch.cuda.amp import autocast\n\n\nclass CTCTrainer(Trainer):\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        if self.use_amp:\n            with autocast():\n                loss = self.compute_loss(model, inputs)\n        else:\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.gradient_accumulation_steps > 1:\n            loss = loss / self.args.gradient_accumulation_steps\n\n        if self.use_amp:\n            self.scaler.scale(loss).backward()\n        elif self.use_apex:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self.deepspeed:\n            self.deepspeed.backward(loss)\n        else:\n            loss.backward()\n\n        return loss.detach()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:40:26.887116Z","iopub.execute_input":"2022-02-21T04:40:26.888022Z","iopub.status.idle":"2022-02-21T04:40:27.031839Z","shell.execute_reply.started":"2022-02-21T04:40:26.887982Z","shell.execute_reply":"2022-02-21T04:40:27.031119Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"/content/wav2vec2-base-100k-eating-sound-collection\",\n    # output_dir=\"/content/gdrive/MyDrive/wav2vec2-base-100k-eating-sound-collection\"\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    evaluation_strategy=\"steps\",\n    num_train_epochs=1.0,\n    fp16=True,\n    save_steps=10,\n    eval_steps=10,\n    logging_steps=10,\n    learning_rate=1e-4,\n    save_total_limit=2,\n)\n\ntraining_args = TrainingArguments(\n    # _n_gpu=1,\n    adafactor=False,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-08,\n    bf16=False,\n    bf16_full_eval=False,\n    dataloader_drop_last=False,\n    dataloader_num_workers=0,\n    dataloader_pin_memory=True,\n    ddp_bucket_cap_mb=None,\n    ddp_find_unused_parameters=None,\n    debug=[],\n    deepspeed=None,\n    disable_tqdm=False,\n    do_eval=True,\n    do_predict=True,\n    do_train=True,\n    eval_accumulation_steps=None,\n    eval_steps=500,\n    evaluation_strategy=\"steps\",\n    fp16=True,\n    fp16_full_eval=False,\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=False,\n    greater_is_better=None,\n    group_by_length=False,\n    ignore_data_skip=False,\n    label_names=None,\n    label_smoothing_factor=0.0,\n    learning_rate=0.0001,\n    load_best_model_at_end=False,\n    local_rank=-1,\n    max_grad_norm=1.0,\n    max_steps=-1,\n    metric_for_best_model=None,\n    no_cuda=False,\n    num_train_epochs=20.0,\n    optim=\"adamw_hf\",\n    output_dir=f\"{save_path}/wav2vec2-base-100k-voxpopuli-gtzan-music\",\n    overwrite_output_dir=True,\n    past_index=-1,\n    per_device_eval_batch_size=1,\n    per_device_train_batch_size=1,\n    prediction_loss_only=False,\n    remove_unused_columns=True,\n    report_to=['tensorboard'],\n    resume_from_checkpoint=None,\n    save_on_each_node=False,\n    save_steps=100,\n    save_total_limit=2,\n    seed=42,\n    sharded_ddp=[],\n    skip_memory_metrics=True,\n    tf32=None,\n    tpu_metrics_debug=False,\n    tpu_num_cores=None,\n    use_legacy_prediction_loop=False,\n    warmup_ratio=0.0,\n    warmup_steps=2000,\n    weight_decay=0.0,\n    xpu_backend=None,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:40:29.406678Z","iopub.execute_input":"2022-02-21T04:40:29.406949Z","iopub.status.idle":"2022-02-21T04:40:29.474124Z","shell.execute_reply.started":"2022-02-21T04:40:29.406919Z","shell.execute_reply":"2022-02-21T04:40:29.473355Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = CTCTrainer(\n    model=model,\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=feature_extractor,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:40:31.129712Z","iopub.execute_input":"2022-02-21T04:40:31.129966Z","iopub.status.idle":"2022-02-21T04:40:35.562841Z","shell.execute_reply.started":"2022-02-21T04:40:31.129939Z","shell.execute_reply":"2022-02-21T04:40:35.562144Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:40:57.418598Z","iopub.execute_input":"2022-02-21T04:40:57.418899Z","iopub.status.idle":"2022-02-21T04:40:57.423675Z","shell.execute_reply.started":"2022-02-21T04:40:57.418856Z","shell.execute_reply":"2022-02-21T04:40:57.422832Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T04:41:01.02631Z","iopub.execute_input":"2022-02-21T04:41:01.026833Z","iopub.status.idle":"2022-02-21T05:01:36.273242Z","shell.execute_reply.started":"2022-02-21T04:41:01.026788Z","shell.execute_reply":"2022-02-21T05:01:36.27234Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport librosa\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2022-02-21T05:03:12.106903Z","iopub.execute_input":"2022-02-21T05:03:12.107377Z","iopub.status.idle":"2022-02-21T05:03:12.117653Z","shell.execute_reply.started":"2022-02-21T05:03:12.107329Z","shell.execute_reply":"2022-02-21T05:03:12.11653Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = load_dataset(\"csv\", data_files={\"test\": \"./test.csv\"})[\"test\"]\ntest_dataset","metadata":{"execution":{"iopub.status.busy":"2022-02-21T05:03:14.460161Z","iopub.execute_input":"2022-02-21T05:03:14.460421Z","iopub.status.idle":"2022-02-21T05:03:14.990307Z","shell.execute_reply.started":"2022-02-21T05:03:14.460394Z","shell.execute_reply":"2022-02-21T05:03:14.989602Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name_or_path = \"m3hrdadfi/wav2vec2-base-100k-gtzan-music-genres\"\nmodel_name_or_path = \"./wav2vec2-base-100k-voxpopuli-gtzan-music/checkpoint-7900\"","metadata":{"execution":{"iopub.status.busy":"2022-02-21T05:03:17.766835Z","iopub.execute_input":"2022-02-21T05:03:17.767405Z","iopub.status.idle":"2022-02-21T05:03:17.771509Z","shell.execute_reply.started":"2022-02-21T05:03:17.767366Z","shell.execute_reply":"2022-02-21T05:03:17.770698Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nconfig = AutoConfig.from_pretrained(model_name_or_path)\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)\nsampling_rate = feature_extractor.sampling_rate\nmodel = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T05:03:19.276767Z","iopub.execute_input":"2022-02-21T05:03:19.277179Z","iopub.status.idle":"2022-02-21T05:03:20.666164Z","shell.execute_reply.started":"2022-02-21T05:03:19.277143Z","shell.execute_reply":"2022-02-21T05:03:20.665375Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    speech_array = speech_array.squeeze().numpy()\n    speech_array = librosa.resample(np.asarray(speech_array), orig_sr=sampling_rate, target_sr=feature_extractor.sampling_rate)\n\n    batch[\"speech\"] = speech_array\n    return batch\n\n\ndef predict(batch):\n    features = feature_extractor(batch[\"speech\"], sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True)\n\n    input_values = features.input_values.to(device)\n\n    with torch.no_grad():\n        logits = model(input_values).logits \n\n    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n    batch[\"predicted\"] = pred_ids\n    return batch","metadata":{"execution":{"iopub.status.busy":"2022-02-21T05:03:32.28563Z","iopub.execute_input":"2022-02-21T05:03:32.285895Z","iopub.status.idle":"2022-02-21T05:03:32.293651Z","shell.execute_reply.started":"2022-02-21T05:03:32.285867Z","shell.execute_reply":"2022-02-21T05:03:32.292927Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = test_dataset.map(speech_file_to_array_fn)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T05:03:34.646824Z","iopub.execute_input":"2022-02-21T05:03:34.647107Z","iopub.status.idle":"2022-02-21T05:04:35.194542Z","shell.execute_reply.started":"2022-02-21T05:03:34.64706Z","shell.execute_reply":"2022-02-21T05:04:35.193637Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = test_dataset.map(predict, batched=True, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T05:04:37.935098Z","iopub.execute_input":"2022-02-21T05:04:37.935359Z","iopub.status.idle":"2022-02-21T05:05:23.412229Z","shell.execute_reply.started":"2022-02-21T05:04:37.935331Z","shell.execute_reply":"2022-02-21T05:05:23.411493Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_names = [config.id2label[i] for i in range(config.num_labels)]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T05:05:26.246487Z","iopub.execute_input":"2022-02-21T05:05:26.246803Z","iopub.status.idle":"2022-02-21T05:05:26.255711Z","shell.execute_reply.started":"2022-02-21T05:05:26.246766Z","shell.execute_reply":"2022-02-21T05:05:26.254894Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true = [config.label2id[name] for name in result[\"label\"]]\ny_pred = result[\"predicted\"]\n\nprint(y_true[:5])\nprint(y_pred[:5])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T05:05:27.991022Z","iopub.execute_input":"2022-02-21T05:05:27.991544Z","iopub.status.idle":"2022-02-21T05:05:27.999795Z","shell.execute_reply.started":"2022-02-21T05:05:27.991504Z","shell.execute_reply":"2022-02-21T05:05:27.998961Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(classification_report(y_true, y_pred, target_names=label_names))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T05:05:31.906681Z","iopub.execute_input":"2022-02-21T05:05:31.907399Z","iopub.status.idle":"2022-02-21T05:05:31.919278Z","shell.execute_reply.started":"2022-02-21T05:05:31.907357Z","shell.execute_reply":"2022-02-21T05:05:31.918312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clean Up","metadata":{}},{"cell_type":"code","source":"!rm -rf cache","metadata":{},"outputs":[],"execution_count":null}]}